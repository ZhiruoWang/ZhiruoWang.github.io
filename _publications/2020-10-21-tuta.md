---
title: "Structure-aware Pre-training for Table Understanding with Tree-based Transformers"
collection: publications
permalink: /publication/2020-10-21-tuta
date: 2020-10-21
paperurl: 'https://arxiv.org/pdf/2010.12537'
citation: 'Zhiruo, Wang, Haoyu Dong, et al. (2020). &quot;Structure-aware Pre-training for Table Understanding with Tree-based Transformers.&quot'
---

[[PDF]](https://arxiv.org/pdf/2010.12537)

## Abstract
Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Since understanding a table needs to leverage both spatial, hierarchical, and semantic information, we adapt the self-attention strategy with several key structure-aware mechanisms. First, we propose a novel tree-based structure called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information in tables. Upon this, we extend the pre-training architecture with two core mechanisms, namely the tree-based attention and tree-based position embedding. Moreover, to capture table information in a progressive manner, we devise three pre-training objectives to enable representations at the token, cell, and table levels. TUTA pre-trains on a wide range of unlabeled tables and fine-tunes on a critical task in the field of table structure understanding, ie cell type classification. Experiment results show that TUTA is highly effective, achieving state-of-the-art on four well-annotated cell type classification datasets.
