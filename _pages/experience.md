---
layout: archive
title: "Experience"
permalink: /experience/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}


## Assistant Researcher
### Microsoft Research Asia (Sep 2020 - Present)
Study and implement the representative works on Semantic Parsing and Program Synthesis
* Experiment under different levels of supervision (weak \& full), using various Reinforcement Learning strategies
* study the Synchronous Context-Free Grammar to generate (canonical) utterances from logical forms (programs)

Investigate top-performing pre-training methods (e.g. TAPAS and TaBERT) for Question Answering on Relational Tables
* Experiment and compare with different table corpus, model structures, tuning strategies, and pre-processing details
* Give a presentation (open to all MSRA interns) of the pre-training architectures leveraged upon real-world tables

Proposed a unified structure-aware pre-training architecture for generally structured tables
* Devised a novel bi-dimensional coordinate tree to encode both the spatial and hierarchical information in tables
* Adapted the attention strategy upon bi-tree distance, boost up&down stream accuray by reducing visible scopes
* Leveraged three pre-training objectives at progressive levels to capture representations of tokens, cells, and tables
* Built the first dedicated vocabulary in the table domain, to embed commonly used table tokens with higher accuracy
* Achieved state-of-the-art performance on all four data sets of a critical downstream task: cell type classification

## Research & Development Intern
### Microsoft Research Asia (Feb 2020 - Aug 2020)
Pre-training on large-scale spreadsheet data towards table intelligence foundation in Microsoft Excel
* Enabled semantic, numeric and format features in table-oriented pre-training
* Studied mix-precision training, factorized parameterization, and memory analysis to expand supporting size

## Machine Learning Engineer Intern
### Tencent (Sep 2019 - Jan 2020)
Optimized BERT-based pre-training network from multiple angles
* Devised a self-distilling pre-training model with adaptive inference time
* Upgraded corpus quality by dig into Emoji pre-processing

## Research Intern
### Tencent AI Lab (Jun 2019 - Aug 2019)
Optimized BERT-based pre-training network from multiple angles
* Injected domain knowledge into models using soft position and visible matrix

## Undergradute Research Program Leader
### State Key Laboratory of Cognitive NeuroScience and Learning (May 2018 - May 2019)
Inspect visual pathways connecting eyes to brain using machine learning algorithms
* Orchestrated novel coding pathways in lightness, contrast and gradient from large-scale Natural Image Dataset
* Effectuated a self-organized map using gradient descent, recovered imange input from cortex stimulus detection

## Leader of the Genetic Engineering Group
### Inter-disciplinary Team of iGEM-BNU (Jan 2018 - Nov 2018)
* Extracted patterns from and guided the desgin of experiements by estimating the population of E.coli
* Captained modeling and designed an antibiotic-free glucose production method in conjuntion with the wet lab

## Model Analyst Intern
### People's Bank of China (Jul 2017 - Aug 2017)
* Generated quantitative analysis by formula re-derivation and structure modification of macro-economical models
* Assisted with governmental decisions regarding programmatic simulations in Central Bank Digital Currency
